{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJEMjPgeI-rw"
      },
      "outputs": [],
      "source": [
        "!pip install moshi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA4K5iDFJcqJ"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/kyutai-labs/moshi/raw/refs/heads/main/data/sample_fr_hibiki_crepes.mp3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA3Haix3IZ8Q"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import time\n",
        "import sentencepiece\n",
        "import sphn\n",
        "import textwrap\n",
        "import torch\n",
        "\n",
        "from moshi.models import loaders, MimiModel, LMModel, LMGen\n",
        "#test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AK5zBMTI9bw"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class InferenceState:\n",
        "    mimi: MimiModel\n",
        "    text_tokenizer: sentencepiece.SentencePieceProcessor\n",
        "    lm_gen: LMGen\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        mimi: MimiModel,\n",
        "        text_tokenizer: sentencepiece.SentencePieceProcessor,\n",
        "        lm: LMModel,\n",
        "        batch_size: int,\n",
        "        device: str | torch.device,\n",
        "    ):\n",
        "        self.mimi = mimi\n",
        "        self.text_tokenizer = text_tokenizer\n",
        "        self.lm_gen = LMGen(lm, temp=0, temp_text=0, use_sampling=False)\n",
        "        self.device = device\n",
        "        self.frame_size = int(self.mimi.sample_rate / self.mimi.frame_rate)\n",
        "        self.batch_size = batch_size\n",
        "        self.mimi.streaming_forever(batch_size)\n",
        "        self.lm_gen.streaming_forever(batch_size)\n",
        "\n",
        "    def run(self, in_pcms: torch.Tensor):\n",
        "        device = self.lm_gen.lm_model.device\n",
        "        ntokens = 0\n",
        "        first_frame = True\n",
        "        chunks = [\n",
        "            c\n",
        "            for c in in_pcms.split(self.frame_size, dim=2)\n",
        "            if c.shape[-1] == self.frame_size\n",
        "        ]\n",
        "        start_time = time.time()\n",
        "        all_text = []\n",
        "        for chunk in chunks:\n",
        "            codes = self.mimi.encode(chunk)\n",
        "            if first_frame:\n",
        "                # Ensure that the first slice of codes is properly seen by the transformer\n",
        "                # as otherwise the first slice is replaced by the initial tokens.\n",
        "                tokens = self.lm_gen.step(codes)\n",
        "                first_frame = False\n",
        "            tokens = self.lm_gen.step(codes)\n",
        "            if tokens is None:\n",
        "                continue\n",
        "            assert tokens.shape[1] == 1\n",
        "            one_text = tokens[0, 0].cpu()\n",
        "            if one_text.item() not in [0, 3]:\n",
        "                text = self.text_tokenizer.id_to_piece(one_text.item())\n",
        "                text = text.replace(\"‚ñÅ\", \" \")\n",
        "                all_text.append(text)\n",
        "            ntokens += 1\n",
        "        dt = time.time() - start_time\n",
        "        print(\n",
        "            f\"processed {ntokens} steps in {dt:.0f}s, {1000 * dt / ntokens:.2f}ms/step\"\n",
        "        )\n",
        "        return \"\".join(all_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsQJdAgkLp9n"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "# Use the en+fr low latency model, an alternative is kyutai/stt-2.6b-en\n",
        "checkpoint_info = loaders.CheckpointInfo.from_hf_repo(\"kyutai/stt-1b-en_fr\")\n",
        "mimi = checkpoint_info.get_mimi(device=device)\n",
        "text_tokenizer = checkpoint_info.get_text_tokenizer()\n",
        "lm = checkpoint_info.get_moshi(device=device)\n",
        "in_pcms, _ = sphn.read(\"sample_fr_hibiki_crepes.mp3\", sample_rate=mimi.sample_rate)\n",
        "in_pcms = torch.from_numpy(in_pcms).to(device=device)\n",
        "\n",
        "stt_config = checkpoint_info.stt_config\n",
        "pad_left = int(stt_config.get(\"audio_silence_prefix_seconds\", 0.0) * 24000)\n",
        "pad_right = int((stt_config.get(\"audio_delay_seconds\", 0.0) + 1.0) * 24000)\n",
        "in_pcms = torch.nn.functional.pad(in_pcms, (pad_left, pad_right), mode=\"constant\")\n",
        "in_pcms = in_pcms[None, 0:1].expand(1, -1, -1)\n",
        "\n",
        "state = InferenceState(mimi, text_tokenizer, lm, batch_size=1, device=device)\n",
        "text = state.run(in_pcms)\n",
        "print(textwrap.fill(text, width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIAXs9oaPrtj"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "Audio(\"sample_fr_hibiki_crepes.mp3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkUZ6CBKOdTa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}